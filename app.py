import streamlit as st
import google.generativeai as genai
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
# Googleã®ãƒ¢ãƒ‡ãƒ«ã¨ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ã†ãŸã‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# --- UIã®åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="My RAG App", layout="wide")
st.title("ğŸ“„ è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼AIãƒãƒ£ãƒƒãƒˆ (Public Ver.)")

# --- APIã‚­ãƒ¼ã®è¨­å®š ---
# Streamlitã®Secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
try:
    genai.configure(api_key=st.secrets["AIzaSyDiEOgaMNj9ERlC_ROrhBY8W-emWqjQV4s"])
except Exception:
    st.error("ã‚¨ãƒ©ãƒ¼: Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlitã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦é«˜é€ŸåŒ–ï¼‰---
@st.cache_resource
def load_models():
    # Ollamaã®ä»£ã‚ã‚Šã«Googleã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
    llm = GoogleGenerativeAI(model="gemini-1.5-flash")
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    return llm, embeddings

llm, embeddings = load_models()

# --- RAGãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹é–¢æ•°ï¼ˆä¸­èº«ã¯ã»ã¼åŒã˜ï¼‰ ---
@st.cache_resource
def create_rag_chain(_file_content):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_text(_file_content)
    
    vectorstore = Chroma.from_texts(texts=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    template = """
    ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã ã‘ã‚’ä½¿ã£ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚

    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
    {context}

    è³ªå•: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# --- UIã®ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ï¼ˆå¤‰æ›´ãªã—ï¼‰ ---
uploaded_files = st.file_uploader(
    "è³ªå•ã—ãŸã„å†…å®¹ãŒæ›¸ã‹ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.txtï¼‰ã‚’è¤‡æ•°é¸æŠã§ãã¾ã™ã€‚", 
    type=["txt"],
    accept_multiple_files=True
)

if uploaded_files:
    all_text = ""
    for uploaded_file in uploaded_files:
        file_content = uploaded_file.getvalue().decode("utf-8")
        all_text += file_content + "\n\n---\n\n"
    
    rag_chain = create_rag_chain(all_text)

    file_names = [f.name for f in uploaded_files]
    st.success(f"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {', '.join(file_names)}")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("AIãŒè€ƒãˆã¦ã„ã¾ã™..."):
                response = rag_chain.invoke(prompt)
                st.markdown(response)
        
        st.session_state.messages.append({"role": "assistant", "content": response})
else:
    st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ãƒãƒ£ãƒƒãƒˆãŒé–‹å§‹ã•ã‚Œã¾ã™ã€‚")
